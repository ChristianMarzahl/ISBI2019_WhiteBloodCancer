{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_spliter import SplitByPatient\n",
    "from metrics import *#F1Weighted, MCC\n",
    "from losses import *\n",
    "from data_loader import ImageItemListCell\n",
    "from augmentation import cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.hooks import  params_size\n",
    "def extract_train_information(learner: Learner):\n",
    "    #_, params, trainables, _ = params_size(learner.model)\n",
    "\n",
    "    #total_params = sum(params)\n",
    "    #trainable_ratio = sum([param*train for param, train in zip(params, trainables)]) / total_params * 100\n",
    "\n",
    "    bs = learner.data.batch_size\n",
    "    image_size = learner.data.valid_ds[0][0].size\n",
    "    wd = learner.wd\n",
    "    lr = max(learner.recorder.lrs)\n",
    "\n",
    "    summary = \"\"\n",
    "    #summary = \"\\nTotal params: {:,}\".format(total_params)\n",
    "    #summary += f\"\\nTrainable: {round(trainable_ratio,2)}%\"\n",
    "    summary += f\"\\nBs: {bs}\"\n",
    "    summary += f\"\\nwd: {wd}\"\n",
    "    summary += f\"\\nlr: {lr}\"\n",
    "    summary += f\"\\nImage: {image_size}\\n\"\n",
    "\n",
    "    for tf in learner.data.train_dl.dl.dataset.tfms:\n",
    "        summary += f\"\\n {tf}\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/data/Datasets/WhiteBloodCancer/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/Datasets/WhiteBloodCancer/train/fold_1/hem/UID_H10_43_1_hem.bmp'),\n",
       " PosixPath('/data/Datasets/WhiteBloodCancer/train/fold_1/hem/UID_H22_31_15_hem.bmp'),\n",
       " PosixPath('/data/Datasets/WhiteBloodCancer/train/fold_1/hem/UID_H14_9_11_hem.bmp'),\n",
       " PosixPath('/data/Datasets/WhiteBloodCancer/train/fold_1/hem/UID_H14_28_6_hem.bmp'),\n",
       " PosixPath('/data/Datasets/WhiteBloodCancer/train/fold_1/hem/UID_H10_189_1_hem.bmp')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = get_image_files(path, recurse=True)\n",
    "fnames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10661"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hem_regex = re.compile(r'UID_(H[0-9]+)_', re.IGNORECASE)\n",
    "all_regex = re.compile(r'UID_([0-9]+)_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hem_patient_ids = list(set([hem_regex.search(str(fn)).group(1)\n",
    "                            for fn in fnames if hem_regex.search(str(fn)) is not None]))\n",
    "all_patint_ids = list(set([all_regex.search(str(fn)).group(1)\n",
    "                           for fn in fnames if all_regex.search(str(fn)) is not None]))\n",
    "\n",
    "hem_patients = dict((k,[]) for k in hem_patient_ids)\n",
    "all_patints = dict((k,[]) for k in all_patint_ids)\n",
    "\n",
    "[all_patints[key].append(fn) for key in all_patints.keys() for fn in fnames if 'UID_{0}_'.format(key) in str(fn)]\n",
    "[hem_patients[key].append(fn) for key in hem_patients.keys() for fn in fnames if 'UID_{0}_'.format(key) in str(fn)]\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_fn = TfmLighting(cutout)\n",
    "xtra_tfms=[squish(scale=0.66), cutout_fn(n_holes=5, length=0.2)]\n",
    "tfms = get_transforms(do_flip=True, \n",
    "                      flip_vert=True, \n",
    "                      max_rotate=90,  \n",
    "                      max_lighting=0.15, \n",
    "                      max_zoom=1.5, \n",
    "                      max_warp=0.2,\n",
    "                      p_affine=0.75,\n",
    "                      p_lighting=0.75,  \n",
    "                      xtra_tfms=xtra_tfms,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = Path('/data/Datasets/WhiteBloodCancer/test/')\n",
    "\n",
    "def get_data(bs, size):\n",
    "    data  = ImageDataBunch.create_from_ll(lls, size=size, bs=bs, \n",
    "                                      ds_tfms=tfms, padding_mode='zeros',\n",
    "                                      resize_method=ResizeMethod.PAD, test=test_path)\n",
    "    data = data.normalize()\n",
    "    #data = data.normalize((channel_mean, channel_std))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r'^.*(hem|all).bmp$')\n",
    "def get_label(fn):\n",
    "    return pat.search(str(fn)).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_handler = SplitByPatient(hem_patients, all_patints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = split_handler.split_by_folds(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "for fold in range(5):\n",
    "    gc.collect()\n",
    "    experiment_name = 'rn34-Image-{}'.format(fold)\n",
    "    size = 450\n",
    "    bs = 128 \n",
    "    \n",
    "    train_files = []\n",
    "    valid_files = []    \n",
    "    train_files += [folds[f] for f in range(5) if f != fold]\n",
    "    valid_files = folds[fold]\n",
    "    \n",
    "    train_files = flatten(train_files)\n",
    "    \n",
    "    if len(train_files+valid_files) != len(fnames):\n",
    "        raise ArithmeticError\n",
    "    \n",
    "    valid = ImageItemList(valid_files)\n",
    "    train = ImageItemList(train_files)\n",
    "    \n",
    "    item_list = ItemLists(path, train, valid)\n",
    "    lls = item_list.label_from_func(get_label)\n",
    "    \n",
    "    learn = create_cnn(get_data(bs, size), models.resnet34, \n",
    "                   metrics=[error_rate, F1Weighted(), MCC()],  \n",
    "                   #loss_func=FocalLoss(num_classes=1, alpha=0.4, gamma=0.5),\n",
    "                   #ps=0.75,\n",
    "                   wd=0.001,\n",
    "                   callback_fns=[ShowGraph, partial(SaveModelCallback, monitor=\"mcc\", mode='max', name='stage1-{}-{}'.format(experiment_name, size))],\n",
    "                  ).to_fp16().mixup()      \n",
    "    \n",
    "    lr = 1e-2\n",
    "    learn.fit_one_cycle(10, lr)\n",
    "    \n",
    "    learn.unfreeze()\n",
    "    learn.callback_fns[2] = partial(SaveModelCallback, \n",
    "                              monitor=\"mcc\", \n",
    "                              mode='max', \n",
    "                              name='stage2-{}-{}'.format(experiment_name, size))\n",
    "    learn.fit_one_cycle(10, slice(1e-5,lr/5))\n",
    "    \n",
    "    preds_test, y_test=learn.get_preds(ds_type=DatasetType.Valid)# \n",
    "    preds_test = np.argmax(torch.sigmoid(preds_test), axis=1)\n",
    "    score = int(matthews_corrcoef(y_test, preds_test) * 100)\n",
    "    \n",
    "    learn.export('{}-{}-{}.pkl'.format(experiment_name, size, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
